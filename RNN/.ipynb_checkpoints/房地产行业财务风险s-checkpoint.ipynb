{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lihanghang/anaconda3/envs/TensorFlow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************************\n",
      "\n",
      "**************************当前是第4列数据,共16列**************************\n",
      "\n",
      "**************************************************************************\n",
      "\n",
      "[[0.81479731]\n",
      " [0.72984199]\n",
      " [0.26095935]\n",
      " [0.27438644]\n",
      " [0.59597564]\n",
      " [1.38416717]\n",
      " [1.59575411]\n",
      " [1.84618745]\n",
      " [1.81554145]\n",
      " [2.37936166]\n",
      " [3.29088238]\n",
      " [3.74076685]\n",
      " [4.50338997]\n",
      " [5.28809505]\n",
      " [5.51065526]\n",
      " [5.72909665]\n",
      " [5.92401827]\n",
      " [5.77068446]\n",
      " [6.84187   ]\n",
      " [7.02052901]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b856939b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/Reshape_1:0\", shape=(?, 5, 10), dtype=float32)\n",
      "Epoch: 0/1000 Iteration: 1 Train loss: 0.016690\n",
      "Epoch: 1/1000 Iteration: 3 Train loss: 0.016107\n",
      "Epoch: 2/1000 Iteration: 5 Train loss: 0.015609\n",
      "Epoch: 3/1000 Iteration: 7 Train loss: 0.015122\n",
      "Epoch: 4/1000 Iteration: 9 Train loss: 0.014649\n",
      "Epoch: 5/1000 Iteration: 11 Train loss: 0.014191\n",
      "Epoch: 6/1000 Iteration: 13 Train loss: 0.013748\n",
      "Epoch: 7/1000 Iteration: 15 Train loss: 0.013321\n",
      "Epoch: 8/1000 Iteration: 17 Train loss: 0.012909\n",
      "Epoch: 9/1000 Iteration: 19 Train loss: 0.012512\n",
      "Epoch: 10/1000 Iteration: 21 Train loss: 0.012131\n",
      "Epoch: 11/1000 Iteration: 23 Train loss: 0.011766\n",
      "Epoch: 12/1000 Iteration: 25 Train loss: 0.011416\n",
      "Epoch: 13/1000 Iteration: 27 Train loss: 0.011081\n",
      "Epoch: 14/1000 Iteration: 29 Train loss: 0.010761\n",
      "Epoch: 15/1000 Iteration: 31 Train loss: 0.010456\n",
      "Epoch: 16/1000 Iteration: 33 Train loss: 0.010166\n",
      "Epoch: 17/1000 Iteration: 35 Train loss: 0.009890\n",
      "Epoch: 18/1000 Iteration: 37 Train loss: 0.009629\n",
      "Epoch: 19/1000 Iteration: 39 Train loss: 0.009381\n",
      "Epoch: 20/1000 Iteration: 41 Train loss: 0.009148\n",
      "Epoch: 21/1000 Iteration: 43 Train loss: 0.008927\n",
      "Epoch: 22/1000 Iteration: 45 Train loss: 0.008720\n",
      "Epoch: 23/1000 Iteration: 47 Train loss: 0.008526\n",
      "Epoch: 24/1000 Iteration: 49 Train loss: 0.008343\n",
      "Epoch: 25/1000 Iteration: 51 Train loss: 0.008174\n",
      "Epoch: 26/1000 Iteration: 53 Train loss: 0.008016\n",
      "Epoch: 27/1000 Iteration: 55 Train loss: 0.007869\n",
      "Epoch: 28/1000 Iteration: 57 Train loss: 0.007733\n",
      "Epoch: 29/1000 Iteration: 59 Train loss: 0.007609\n",
      "Epoch: 30/1000 Iteration: 61 Train loss: 0.007495\n",
      "Epoch: 31/1000 Iteration: 63 Train loss: 0.007391\n",
      "Epoch: 32/1000 Iteration: 65 Train loss: 0.007296\n",
      "Epoch: 33/1000 Iteration: 67 Train loss: 0.007211\n",
      "Epoch: 34/1000 Iteration: 69 Train loss: 0.007136\n",
      "Epoch: 35/1000 Iteration: 71 Train loss: 0.007069\n",
      "Epoch: 36/1000 Iteration: 73 Train loss: 0.007010\n",
      "Epoch: 37/1000 Iteration: 75 Train loss: 0.006959\n",
      "Epoch: 38/1000 Iteration: 77 Train loss: 0.006917\n",
      "Epoch: 39/1000 Iteration: 79 Train loss: 0.006881\n",
      "Epoch: 40/1000 Iteration: 81 Train loss: 0.006853\n",
      "Epoch: 41/1000 Iteration: 83 Train loss: 0.006831\n",
      "Epoch: 42/1000 Iteration: 85 Train loss: 0.006816\n",
      "Epoch: 43/1000 Iteration: 87 Train loss: 0.006807\n",
      "Epoch: 44/1000 Iteration: 89 Train loss: 0.006804\n",
      "Epoch: 45/1000 Iteration: 91 Train loss: 0.006807\n",
      "Epoch: 46/1000 Iteration: 93 Train loss: 0.006814\n",
      "Epoch: 47/1000 Iteration: 95 Train loss: 0.006827\n",
      "Epoch: 48/1000 Iteration: 97 Train loss: 0.006844\n",
      "Epoch: 49/1000 Iteration: 99 Train loss: 0.006866\n",
      "Epoch: 50/1000 Iteration: 101 Train loss: 0.006891\n",
      "Epoch: 51/1000 Iteration: 103 Train loss: 0.006921\n",
      "Epoch: 52/1000 Iteration: 105 Train loss: 0.006954\n",
      "Epoch: 53/1000 Iteration: 107 Train loss: 0.006990\n",
      "Epoch: 54/1000 Iteration: 109 Train loss: 0.007029\n",
      "Epoch: 55/1000 Iteration: 111 Train loss: 0.007071\n",
      "Epoch: 56/1000 Iteration: 113 Train loss: 0.007116\n",
      "Epoch: 57/1000 Iteration: 115 Train loss: 0.007163\n",
      "Epoch: 58/1000 Iteration: 117 Train loss: 0.007213\n",
      "Epoch: 59/1000 Iteration: 119 Train loss: 0.007264\n",
      "Epoch: 60/1000 Iteration: 121 Train loss: 0.007317\n",
      "Epoch: 61/1000 Iteration: 123 Train loss: 0.007371\n",
      "Epoch: 62/1000 Iteration: 125 Train loss: 0.007427\n",
      "Epoch: 63/1000 Iteration: 127 Train loss: 0.007484\n",
      "Epoch: 64/1000 Iteration: 129 Train loss: 0.007542\n",
      "Epoch: 65/1000 Iteration: 131 Train loss: 0.007600\n",
      "Epoch: 66/1000 Iteration: 133 Train loss: 0.007660\n",
      "Epoch: 67/1000 Iteration: 135 Train loss: 0.007720\n",
      "Epoch: 68/1000 Iteration: 137 Train loss: 0.007780\n",
      "Epoch: 69/1000 Iteration: 139 Train loss: 0.007840\n",
      "Epoch: 70/1000 Iteration: 141 Train loss: 0.007901\n",
      "Epoch: 71/1000 Iteration: 143 Train loss: 0.007961\n",
      "Epoch: 72/1000 Iteration: 145 Train loss: 0.008022\n",
      "Epoch: 73/1000 Iteration: 147 Train loss: 0.008082\n",
      "Epoch: 74/1000 Iteration: 149 Train loss: 0.008142\n",
      "Epoch: 75/1000 Iteration: 151 Train loss: 0.008201\n",
      "Epoch: 76/1000 Iteration: 153 Train loss: 0.008259\n",
      "Epoch: 77/1000 Iteration: 155 Train loss: 0.008317\n",
      "Epoch: 78/1000 Iteration: 157 Train loss: 0.008374\n",
      "Epoch: 79/1000 Iteration: 159 Train loss: 0.008431\n",
      "Epoch: 80/1000 Iteration: 161 Train loss: 0.008486\n",
      "Epoch: 81/1000 Iteration: 163 Train loss: 0.008541\n",
      "Epoch: 82/1000 Iteration: 165 Train loss: 0.008594\n",
      "Epoch: 83/1000 Iteration: 167 Train loss: 0.008646\n",
      "Epoch: 84/1000 Iteration: 169 Train loss: 0.008698\n",
      "Epoch: 85/1000 Iteration: 171 Train loss: 0.008747\n",
      "Epoch: 86/1000 Iteration: 173 Train loss: 0.008796\n",
      "Epoch: 87/1000 Iteration: 175 Train loss: 0.008844\n",
      "Epoch: 88/1000 Iteration: 177 Train loss: 0.008890\n",
      "Epoch: 89/1000 Iteration: 179 Train loss: 0.008934\n",
      "Epoch: 90/1000 Iteration: 181 Train loss: 0.008978\n",
      "Epoch: 91/1000 Iteration: 183 Train loss: 0.009019\n",
      "Epoch: 92/1000 Iteration: 185 Train loss: 0.009060\n",
      "Epoch: 93/1000 Iteration: 187 Train loss: 0.009099\n",
      "Epoch: 94/1000 Iteration: 189 Train loss: 0.009136\n",
      "Epoch: 95/1000 Iteration: 191 Train loss: 0.009172\n",
      "Epoch: 96/1000 Iteration: 193 Train loss: 0.009207\n",
      "Epoch: 97/1000 Iteration: 195 Train loss: 0.009240\n",
      "Epoch: 98/1000 Iteration: 197 Train loss: 0.009271\n",
      "Epoch: 99/1000 Iteration: 199 Train loss: 0.009301\n",
      "Epoch: 100/1000 Iteration: 201 Train loss: 0.009329\n",
      "Epoch: 101/1000 Iteration: 203 Train loss: 0.009356\n",
      "Epoch: 102/1000 Iteration: 205 Train loss: 0.009381\n",
      "Epoch: 103/1000 Iteration: 207 Train loss: 0.009405\n",
      "Epoch: 104/1000 Iteration: 209 Train loss: 0.009427\n",
      "Epoch: 105/1000 Iteration: 211 Train loss: 0.009448\n",
      "Epoch: 106/1000 Iteration: 213 Train loss: 0.009468\n",
      "Epoch: 107/1000 Iteration: 215 Train loss: 0.009485\n",
      "Epoch: 108/1000 Iteration: 217 Train loss: 0.009502\n",
      "Epoch: 109/1000 Iteration: 219 Train loss: 0.009517\n",
      "Epoch: 110/1000 Iteration: 221 Train loss: 0.009530\n",
      "Epoch: 111/1000 Iteration: 223 Train loss: 0.009542\n",
      "Epoch: 112/1000 Iteration: 225 Train loss: 0.009553\n",
      "Epoch: 113/1000 Iteration: 227 Train loss: 0.009562\n",
      "Epoch: 114/1000 Iteration: 229 Train loss: 0.009570\n",
      "Epoch: 115/1000 Iteration: 231 Train loss: 0.009577\n",
      "Epoch: 116/1000 Iteration: 233 Train loss: 0.009582\n",
      "Epoch: 117/1000 Iteration: 235 Train loss: 0.009586\n",
      "Epoch: 118/1000 Iteration: 237 Train loss: 0.009589\n",
      "Epoch: 119/1000 Iteration: 239 Train loss: 0.009590\n",
      "Epoch: 120/1000 Iteration: 241 Train loss: 0.009591\n",
      "Epoch: 121/1000 Iteration: 243 Train loss: 0.009590\n",
      "Epoch: 122/1000 Iteration: 245 Train loss: 0.009588\n",
      "Epoch: 123/1000 Iteration: 247 Train loss: 0.009585\n",
      "Epoch: 124/1000 Iteration: 249 Train loss: 0.009580\n",
      "Epoch: 125/1000 Iteration: 251 Train loss: 0.009575\n",
      "Epoch: 126/1000 Iteration: 253 Train loss: 0.009569\n",
      "Epoch: 127/1000 Iteration: 255 Train loss: 0.009561\n",
      "Epoch: 128/1000 Iteration: 257 Train loss: 0.009553\n",
      "Epoch: 129/1000 Iteration: 259 Train loss: 0.009543\n",
      "Epoch: 130/1000 Iteration: 261 Train loss: 0.009533\n",
      "Epoch: 131/1000 Iteration: 263 Train loss: 0.009522\n",
      "Epoch: 132/1000 Iteration: 265 Train loss: 0.009509\n",
      "Epoch: 133/1000 Iteration: 267 Train loss: 0.009496\n",
      "Epoch: 134/1000 Iteration: 269 Train loss: 0.009482\n",
      "Epoch: 135/1000 Iteration: 271 Train loss: 0.009468\n",
      "Epoch: 136/1000 Iteration: 273 Train loss: 0.009452\n",
      "Epoch: 137/1000 Iteration: 275 Train loss: 0.009436\n",
      "Epoch: 138/1000 Iteration: 277 Train loss: 0.009419\n",
      "Epoch: 139/1000 Iteration: 279 Train loss: 0.009401\n",
      "Epoch: 140/1000 Iteration: 281 Train loss: 0.009382\n",
      "Epoch: 141/1000 Iteration: 283 Train loss: 0.009363\n",
      "Epoch: 142/1000 Iteration: 285 Train loss: 0.009343\n",
      "Epoch: 143/1000 Iteration: 287 Train loss: 0.009323\n",
      "Epoch: 144/1000 Iteration: 289 Train loss: 0.009302\n",
      "Epoch: 145/1000 Iteration: 291 Train loss: 0.009280\n",
      "Epoch: 146/1000 Iteration: 293 Train loss: 0.009258\n",
      "Epoch: 147/1000 Iteration: 295 Train loss: 0.009235\n",
      "Epoch: 148/1000 Iteration: 297 Train loss: 0.009212\n",
      "Epoch: 149/1000 Iteration: 299 Train loss: 0.009189\n",
      "Epoch: 150/1000 Iteration: 301 Train loss: 0.009164\n",
      "Epoch: 151/1000 Iteration: 303 Train loss: 0.009140\n",
      "Epoch: 152/1000 Iteration: 305 Train loss: 0.009115\n",
      "Epoch: 153/1000 Iteration: 307 Train loss: 0.009089\n",
      "Epoch: 154/1000 Iteration: 309 Train loss: 0.009064\n",
      "Epoch: 155/1000 Iteration: 311 Train loss: 0.009037\n",
      "Epoch: 156/1000 Iteration: 313 Train loss: 0.009011\n",
      "Epoch: 157/1000 Iteration: 315 Train loss: 0.008984\n",
      "Epoch: 158/1000 Iteration: 317 Train loss: 0.008957\n",
      "Epoch: 159/1000 Iteration: 319 Train loss: 0.008929\n",
      "Epoch: 160/1000 Iteration: 321 Train loss: 0.008902\n",
      "Epoch: 161/1000 Iteration: 323 Train loss: 0.008874\n",
      "Epoch: 162/1000 Iteration: 325 Train loss: 0.008845\n",
      "Epoch: 163/1000 Iteration: 327 Train loss: 0.008817\n",
      "Epoch: 164/1000 Iteration: 329 Train loss: 0.008788\n",
      "Epoch: 165/1000 Iteration: 331 Train loss: 0.008759\n",
      "Epoch: 166/1000 Iteration: 333 Train loss: 0.008730\n",
      "Epoch: 167/1000 Iteration: 335 Train loss: 0.008701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 168/1000 Iteration: 337 Train loss: 0.008671\n",
      "Epoch: 169/1000 Iteration: 339 Train loss: 0.008642\n",
      "Epoch: 170/1000 Iteration: 341 Train loss: 0.008612\n",
      "Epoch: 171/1000 Iteration: 343 Train loss: 0.008582\n",
      "Epoch: 172/1000 Iteration: 345 Train loss: 0.008552\n",
      "Epoch: 173/1000 Iteration: 347 Train loss: 0.008522\n",
      "Epoch: 174/1000 Iteration: 349 Train loss: 0.008492\n",
      "Epoch: 175/1000 Iteration: 351 Train loss: 0.008462\n",
      "Epoch: 176/1000 Iteration: 353 Train loss: 0.008431\n",
      "Epoch: 177/1000 Iteration: 355 Train loss: 0.008401\n",
      "Epoch: 178/1000 Iteration: 357 Train loss: 0.008371\n",
      "Epoch: 179/1000 Iteration: 359 Train loss: 0.008340\n",
      "Epoch: 180/1000 Iteration: 361 Train loss: 0.008310\n",
      "Epoch: 181/1000 Iteration: 363 Train loss: 0.008279\n",
      "Epoch: 182/1000 Iteration: 365 Train loss: 0.008249\n",
      "Epoch: 183/1000 Iteration: 367 Train loss: 0.008218\n",
      "Epoch: 184/1000 Iteration: 369 Train loss: 0.008188\n",
      "Epoch: 185/1000 Iteration: 371 Train loss: 0.008157\n",
      "Epoch: 186/1000 Iteration: 373 Train loss: 0.008127\n",
      "Epoch: 187/1000 Iteration: 375 Train loss: 0.008097\n",
      "Epoch: 188/1000 Iteration: 377 Train loss: 0.008066\n",
      "Epoch: 189/1000 Iteration: 379 Train loss: 0.008036\n",
      "Epoch: 190/1000 Iteration: 381 Train loss: 0.008006\n",
      "Epoch: 191/1000 Iteration: 383 Train loss: 0.007975\n",
      "Epoch: 192/1000 Iteration: 385 Train loss: 0.007945\n",
      "Epoch: 193/1000 Iteration: 387 Train loss: 0.007915\n",
      "Epoch: 194/1000 Iteration: 389 Train loss: 0.007885\n",
      "Epoch: 195/1000 Iteration: 391 Train loss: 0.007855\n",
      "Epoch: 196/1000 Iteration: 393 Train loss: 0.007825\n",
      "Epoch: 197/1000 Iteration: 395 Train loss: 0.007795\n",
      "Epoch: 198/1000 Iteration: 397 Train loss: 0.007766\n",
      "Epoch: 199/1000 Iteration: 399 Train loss: 0.007736\n",
      "Epoch: 200/1000 Iteration: 401 Train loss: 0.007707\n",
      "Epoch: 201/1000 Iteration: 403 Train loss: 0.007677\n",
      "Epoch: 202/1000 Iteration: 405 Train loss: 0.007648\n",
      "Epoch: 203/1000 Iteration: 407 Train loss: 0.007619\n",
      "Epoch: 204/1000 Iteration: 409 Train loss: 0.007590\n",
      "Epoch: 205/1000 Iteration: 411 Train loss: 0.007561\n",
      "Epoch: 206/1000 Iteration: 413 Train loss: 0.007532\n",
      "Epoch: 207/1000 Iteration: 415 Train loss: 0.007503\n",
      "Epoch: 208/1000 Iteration: 417 Train loss: 0.007475\n",
      "Epoch: 209/1000 Iteration: 419 Train loss: 0.007446\n",
      "Epoch: 210/1000 Iteration: 421 Train loss: 0.007418\n",
      "Epoch: 211/1000 Iteration: 423 Train loss: 0.007390\n",
      "Epoch: 212/1000 Iteration: 425 Train loss: 0.007362\n",
      "Epoch: 213/1000 Iteration: 427 Train loss: 0.007334\n",
      "Epoch: 214/1000 Iteration: 429 Train loss: 0.007306\n",
      "Epoch: 215/1000 Iteration: 431 Train loss: 0.007279\n",
      "Epoch: 216/1000 Iteration: 433 Train loss: 0.007252\n",
      "Epoch: 217/1000 Iteration: 435 Train loss: 0.007224\n",
      "Epoch: 218/1000 Iteration: 437 Train loss: 0.007197\n",
      "Epoch: 219/1000 Iteration: 439 Train loss: 0.007170\n",
      "Epoch: 220/1000 Iteration: 441 Train loss: 0.007143\n",
      "Epoch: 221/1000 Iteration: 443 Train loss: 0.007117\n",
      "Epoch: 222/1000 Iteration: 445 Train loss: 0.007090\n",
      "Epoch: 223/1000 Iteration: 447 Train loss: 0.007064\n",
      "Epoch: 224/1000 Iteration: 449 Train loss: 0.007038\n",
      "Epoch: 225/1000 Iteration: 451 Train loss: 0.007012\n",
      "Epoch: 226/1000 Iteration: 453 Train loss: 0.006986\n",
      "Epoch: 227/1000 Iteration: 455 Train loss: 0.006960\n",
      "Epoch: 228/1000 Iteration: 457 Train loss: 0.006935\n",
      "Epoch: 229/1000 Iteration: 459 Train loss: 0.006909\n",
      "Epoch: 230/1000 Iteration: 461 Train loss: 0.006884\n",
      "Epoch: 231/1000 Iteration: 463 Train loss: 0.006859\n",
      "Epoch: 232/1000 Iteration: 465 Train loss: 0.006834\n",
      "Epoch: 233/1000 Iteration: 467 Train loss: 0.006810\n",
      "Epoch: 234/1000 Iteration: 469 Train loss: 0.006785\n",
      "Epoch: 235/1000 Iteration: 471 Train loss: 0.006761\n",
      "Epoch: 236/1000 Iteration: 473 Train loss: 0.006737\n",
      "Epoch: 237/1000 Iteration: 475 Train loss: 0.006713\n",
      "Epoch: 238/1000 Iteration: 477 Train loss: 0.006689\n",
      "Epoch: 239/1000 Iteration: 479 Train loss: 0.006665\n",
      "Epoch: 240/1000 Iteration: 481 Train loss: 0.006642\n",
      "Epoch: 241/1000 Iteration: 483 Train loss: 0.006618\n",
      "Epoch: 242/1000 Iteration: 485 Train loss: 0.006595\n",
      "Epoch: 243/1000 Iteration: 487 Train loss: 0.006572\n",
      "Epoch: 244/1000 Iteration: 489 Train loss: 0.006549\n",
      "Epoch: 245/1000 Iteration: 491 Train loss: 0.006527\n",
      "Epoch: 246/1000 Iteration: 493 Train loss: 0.006504\n",
      "Epoch: 247/1000 Iteration: 495 Train loss: 0.006482\n",
      "Epoch: 248/1000 Iteration: 497 Train loss: 0.006460\n",
      "Epoch: 249/1000 Iteration: 499 Train loss: 0.006438\n",
      "Epoch: 250/1000 Iteration: 501 Train loss: 0.006416\n",
      "Epoch: 251/1000 Iteration: 503 Train loss: 0.006394\n",
      "Epoch: 252/1000 Iteration: 505 Train loss: 0.006373\n",
      "Epoch: 253/1000 Iteration: 507 Train loss: 0.006352\n",
      "Epoch: 254/1000 Iteration: 509 Train loss: 0.006331\n",
      "Epoch: 255/1000 Iteration: 511 Train loss: 0.006310\n",
      "Epoch: 256/1000 Iteration: 513 Train loss: 0.006289\n",
      "Epoch: 257/1000 Iteration: 515 Train loss: 0.006268\n",
      "Epoch: 258/1000 Iteration: 517 Train loss: 0.006248\n",
      "Epoch: 259/1000 Iteration: 519 Train loss: 0.006228\n",
      "Epoch: 260/1000 Iteration: 521 Train loss: 0.006207\n",
      "Epoch: 261/1000 Iteration: 523 Train loss: 0.006187\n",
      "Epoch: 262/1000 Iteration: 525 Train loss: 0.006168\n",
      "Epoch: 263/1000 Iteration: 527 Train loss: 0.006148\n",
      "Epoch: 264/1000 Iteration: 529 Train loss: 0.006128\n",
      "Epoch: 265/1000 Iteration: 531 Train loss: 0.006109\n",
      "Epoch: 266/1000 Iteration: 533 Train loss: 0.006090\n",
      "Epoch: 267/1000 Iteration: 535 Train loss: 0.006071\n",
      "Epoch: 268/1000 Iteration: 537 Train loss: 0.006052\n",
      "Epoch: 269/1000 Iteration: 539 Train loss: 0.006034\n",
      "Epoch: 270/1000 Iteration: 541 Train loss: 0.006015\n",
      "Epoch: 271/1000 Iteration: 543 Train loss: 0.005997\n",
      "Epoch: 272/1000 Iteration: 545 Train loss: 0.005978\n",
      "Epoch: 273/1000 Iteration: 547 Train loss: 0.005960\n",
      "Epoch: 274/1000 Iteration: 549 Train loss: 0.005943\n",
      "Epoch: 275/1000 Iteration: 551 Train loss: 0.005925\n",
      "Epoch: 276/1000 Iteration: 553 Train loss: 0.005907\n",
      "Epoch: 277/1000 Iteration: 555 Train loss: 0.005890\n",
      "Epoch: 278/1000 Iteration: 557 Train loss: 0.005873\n",
      "Epoch: 279/1000 Iteration: 559 Train loss: 0.005855\n",
      "Epoch: 280/1000 Iteration: 561 Train loss: 0.005839\n",
      "Epoch: 281/1000 Iteration: 563 Train loss: 0.005822\n",
      "Epoch: 282/1000 Iteration: 565 Train loss: 0.005805\n",
      "Epoch: 283/1000 Iteration: 567 Train loss: 0.005789\n",
      "Epoch: 284/1000 Iteration: 569 Train loss: 0.005772\n",
      "Epoch: 285/1000 Iteration: 571 Train loss: 0.005756\n",
      "Epoch: 286/1000 Iteration: 573 Train loss: 0.005740\n",
      "Epoch: 287/1000 Iteration: 575 Train loss: 0.005724\n",
      "Epoch: 288/1000 Iteration: 577 Train loss: 0.005708\n",
      "Epoch: 289/1000 Iteration: 579 Train loss: 0.005693\n",
      "Epoch: 290/1000 Iteration: 581 Train loss: 0.005677\n",
      "Epoch: 291/1000 Iteration: 583 Train loss: 0.005662\n",
      "Epoch: 292/1000 Iteration: 585 Train loss: 0.005647\n",
      "Epoch: 293/1000 Iteration: 587 Train loss: 0.005632\n",
      "Epoch: 294/1000 Iteration: 589 Train loss: 0.005617\n",
      "Epoch: 295/1000 Iteration: 591 Train loss: 0.005602\n",
      "Epoch: 296/1000 Iteration: 593 Train loss: 0.005587\n",
      "Epoch: 297/1000 Iteration: 595 Train loss: 0.005573\n",
      "Epoch: 298/1000 Iteration: 597 Train loss: 0.005558\n",
      "Epoch: 299/1000 Iteration: 599 Train loss: 0.005544\n",
      "Epoch: 300/1000 Iteration: 601 Train loss: 0.005530\n",
      "Epoch: 301/1000 Iteration: 603 Train loss: 0.005516\n",
      "Epoch: 302/1000 Iteration: 605 Train loss: 0.005502\n",
      "Epoch: 303/1000 Iteration: 607 Train loss: 0.005489\n",
      "Epoch: 304/1000 Iteration: 609 Train loss: 0.005475\n",
      "Epoch: 305/1000 Iteration: 611 Train loss: 0.005462\n",
      "Epoch: 306/1000 Iteration: 613 Train loss: 0.005448\n",
      "Epoch: 307/1000 Iteration: 615 Train loss: 0.005435\n",
      "Epoch: 308/1000 Iteration: 617 Train loss: 0.005422\n",
      "Epoch: 309/1000 Iteration: 619 Train loss: 0.005409\n",
      "Epoch: 310/1000 Iteration: 621 Train loss: 0.005396\n",
      "Epoch: 311/1000 Iteration: 623 Train loss: 0.005384\n",
      "Epoch: 312/1000 Iteration: 625 Train loss: 0.005371\n",
      "Epoch: 313/1000 Iteration: 627 Train loss: 0.005359\n",
      "Epoch: 314/1000 Iteration: 629 Train loss: 0.005347\n",
      "Epoch: 315/1000 Iteration: 631 Train loss: 0.005334\n",
      "Epoch: 316/1000 Iteration: 633 Train loss: 0.005322\n",
      "Epoch: 317/1000 Iteration: 635 Train loss: 0.005310\n",
      "Epoch: 318/1000 Iteration: 637 Train loss: 0.005299\n",
      "Epoch: 319/1000 Iteration: 639 Train loss: 0.005287\n",
      "Epoch: 320/1000 Iteration: 641 Train loss: 0.005275\n",
      "Epoch: 321/1000 Iteration: 643 Train loss: 0.005264\n",
      "Epoch: 322/1000 Iteration: 645 Train loss: 0.005253\n",
      "Epoch: 323/1000 Iteration: 647 Train loss: 0.005241\n",
      "Epoch: 324/1000 Iteration: 649 Train loss: 0.005230\n",
      "Epoch: 325/1000 Iteration: 651 Train loss: 0.005219\n",
      "Epoch: 326/1000 Iteration: 653 Train loss: 0.005208\n",
      "Epoch: 327/1000 Iteration: 655 Train loss: 0.005198\n",
      "Epoch: 328/1000 Iteration: 657 Train loss: 0.005187\n",
      "Epoch: 329/1000 Iteration: 659 Train loss: 0.005176\n",
      "Epoch: 330/1000 Iteration: 661 Train loss: 0.005166\n",
      "Epoch: 331/1000 Iteration: 663 Train loss: 0.005156\n",
      "Epoch: 332/1000 Iteration: 665 Train loss: 0.005146\n",
      "Epoch: 333/1000 Iteration: 667 Train loss: 0.005135\n",
      "Epoch: 334/1000 Iteration: 669 Train loss: 0.005125\n",
      "Epoch: 335/1000 Iteration: 671 Train loss: 0.005116\n",
      "Epoch: 336/1000 Iteration: 673 Train loss: 0.005106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 337/1000 Iteration: 675 Train loss: 0.005096\n",
      "Epoch: 338/1000 Iteration: 677 Train loss: 0.005086\n",
      "Epoch: 339/1000 Iteration: 679 Train loss: 0.005077\n",
      "Epoch: 340/1000 Iteration: 681 Train loss: 0.005068\n",
      "Epoch: 341/1000 Iteration: 683 Train loss: 0.005058\n",
      "Epoch: 342/1000 Iteration: 685 Train loss: 0.005049\n",
      "Epoch: 343/1000 Iteration: 687 Train loss: 0.005040\n",
      "Epoch: 344/1000 Iteration: 689 Train loss: 0.005031\n",
      "Epoch: 345/1000 Iteration: 691 Train loss: 0.005022\n",
      "Epoch: 346/1000 Iteration: 693 Train loss: 0.005013\n",
      "Epoch: 347/1000 Iteration: 695 Train loss: 0.005005\n",
      "Epoch: 348/1000 Iteration: 697 Train loss: 0.004996\n",
      "Epoch: 349/1000 Iteration: 699 Train loss: 0.004988\n",
      "Epoch: 350/1000 Iteration: 701 Train loss: 0.004979\n",
      "Epoch: 351/1000 Iteration: 703 Train loss: 0.004971\n",
      "Epoch: 352/1000 Iteration: 705 Train loss: 0.004963\n",
      "Epoch: 353/1000 Iteration: 707 Train loss: 0.004955\n",
      "Epoch: 354/1000 Iteration: 709 Train loss: 0.004947\n",
      "Epoch: 355/1000 Iteration: 711 Train loss: 0.004939\n",
      "Epoch: 356/1000 Iteration: 713 Train loss: 0.004931\n",
      "Epoch: 357/1000 Iteration: 715 Train loss: 0.004923\n",
      "Epoch: 358/1000 Iteration: 717 Train loss: 0.004915\n",
      "Epoch: 359/1000 Iteration: 719 Train loss: 0.004908\n",
      "Epoch: 360/1000 Iteration: 721 Train loss: 0.004900\n",
      "Epoch: 361/1000 Iteration: 723 Train loss: 0.004893\n",
      "Epoch: 362/1000 Iteration: 725 Train loss: 0.004885\n",
      "Epoch: 363/1000 Iteration: 727 Train loss: 0.004878\n",
      "Epoch: 364/1000 Iteration: 729 Train loss: 0.004871\n",
      "Epoch: 365/1000 Iteration: 731 Train loss: 0.004864\n",
      "Epoch: 366/1000 Iteration: 733 Train loss: 0.004857\n",
      "Epoch: 367/1000 Iteration: 735 Train loss: 0.004850\n",
      "Epoch: 368/1000 Iteration: 737 Train loss: 0.004843\n",
      "Epoch: 369/1000 Iteration: 739 Train loss: 0.004836\n",
      "Epoch: 370/1000 Iteration: 741 Train loss: 0.004829\n",
      "Epoch: 371/1000 Iteration: 743 Train loss: 0.004823\n",
      "Epoch: 372/1000 Iteration: 745 Train loss: 0.004816\n",
      "Epoch: 373/1000 Iteration: 747 Train loss: 0.004810\n",
      "Epoch: 374/1000 Iteration: 749 Train loss: 0.004803\n",
      "Epoch: 375/1000 Iteration: 751 Train loss: 0.004797\n",
      "Epoch: 376/1000 Iteration: 753 Train loss: 0.004791\n",
      "Epoch: 377/1000 Iteration: 755 Train loss: 0.004785\n",
      "Epoch: 378/1000 Iteration: 757 Train loss: 0.004778\n",
      "Epoch: 379/1000 Iteration: 759 Train loss: 0.004772\n",
      "Epoch: 380/1000 Iteration: 761 Train loss: 0.004766\n",
      "Epoch: 381/1000 Iteration: 763 Train loss: 0.004760\n",
      "Epoch: 382/1000 Iteration: 765 Train loss: 0.004755\n",
      "Epoch: 383/1000 Iteration: 767 Train loss: 0.004749\n",
      "Epoch: 384/1000 Iteration: 769 Train loss: 0.004743\n",
      "Epoch: 385/1000 Iteration: 771 Train loss: 0.004738\n",
      "Epoch: 386/1000 Iteration: 773 Train loss: 0.004732\n",
      "Epoch: 387/1000 Iteration: 775 Train loss: 0.004726\n",
      "Epoch: 388/1000 Iteration: 777 Train loss: 0.004721\n",
      "Epoch: 389/1000 Iteration: 779 Train loss: 0.004716\n",
      "Epoch: 390/1000 Iteration: 781 Train loss: 0.004710\n",
      "Epoch: 391/1000 Iteration: 783 Train loss: 0.004705\n",
      "Epoch: 392/1000 Iteration: 785 Train loss: 0.004700\n",
      "Epoch: 393/1000 Iteration: 787 Train loss: 0.004695\n",
      "Epoch: 394/1000 Iteration: 789 Train loss: 0.004690\n",
      "Epoch: 395/1000 Iteration: 791 Train loss: 0.004685\n",
      "Epoch: 396/1000 Iteration: 793 Train loss: 0.004680\n",
      "Epoch: 397/1000 Iteration: 795 Train loss: 0.004675\n",
      "Epoch: 398/1000 Iteration: 797 Train loss: 0.004670\n",
      "Epoch: 399/1000 Iteration: 799 Train loss: 0.004665\n",
      "Epoch: 400/1000 Iteration: 801 Train loss: 0.004661\n",
      "Epoch: 401/1000 Iteration: 803 Train loss: 0.004656\n",
      "Epoch: 402/1000 Iteration: 805 Train loss: 0.004651\n",
      "Epoch: 403/1000 Iteration: 807 Train loss: 0.004647\n",
      "Epoch: 404/1000 Iteration: 809 Train loss: 0.004642\n",
      "Epoch: 405/1000 Iteration: 811 Train loss: 0.004638\n",
      "Epoch: 406/1000 Iteration: 813 Train loss: 0.004633\n",
      "Epoch: 407/1000 Iteration: 815 Train loss: 0.004629\n",
      "Epoch: 408/1000 Iteration: 817 Train loss: 0.004625\n",
      "Epoch: 409/1000 Iteration: 819 Train loss: 0.004621\n",
      "Epoch: 410/1000 Iteration: 821 Train loss: 0.004616\n",
      "Epoch: 411/1000 Iteration: 823 Train loss: 0.004612\n",
      "Epoch: 412/1000 Iteration: 825 Train loss: 0.004608\n",
      "Epoch: 413/1000 Iteration: 827 Train loss: 0.004604\n",
      "Epoch: 414/1000 Iteration: 829 Train loss: 0.004600\n",
      "Epoch: 415/1000 Iteration: 831 Train loss: 0.004596\n",
      "Epoch: 416/1000 Iteration: 833 Train loss: 0.004592\n",
      "Epoch: 417/1000 Iteration: 835 Train loss: 0.004589\n",
      "Epoch: 418/1000 Iteration: 837 Train loss: 0.004585\n",
      "Epoch: 419/1000 Iteration: 839 Train loss: 0.004581\n",
      "Epoch: 420/1000 Iteration: 841 Train loss: 0.004577\n",
      "Epoch: 421/1000 Iteration: 843 Train loss: 0.004574\n",
      "Epoch: 422/1000 Iteration: 845 Train loss: 0.004570\n",
      "Epoch: 423/1000 Iteration: 847 Train loss: 0.004567\n",
      "Epoch: 424/1000 Iteration: 849 Train loss: 0.004563\n",
      "Epoch: 425/1000 Iteration: 851 Train loss: 0.004560\n",
      "Epoch: 426/1000 Iteration: 853 Train loss: 0.004556\n",
      "Epoch: 427/1000 Iteration: 855 Train loss: 0.004553\n",
      "Epoch: 428/1000 Iteration: 857 Train loss: 0.004550\n",
      "Epoch: 429/1000 Iteration: 859 Train loss: 0.004546\n",
      "Epoch: 430/1000 Iteration: 861 Train loss: 0.004543\n",
      "Epoch: 431/1000 Iteration: 863 Train loss: 0.004540\n",
      "Epoch: 432/1000 Iteration: 865 Train loss: 0.004537\n",
      "Epoch: 433/1000 Iteration: 867 Train loss: 0.004533\n",
      "Epoch: 434/1000 Iteration: 869 Train loss: 0.004530\n",
      "Epoch: 435/1000 Iteration: 871 Train loss: 0.004527\n",
      "Epoch: 436/1000 Iteration: 873 Train loss: 0.004524\n",
      "Epoch: 437/1000 Iteration: 875 Train loss: 0.004521\n",
      "Epoch: 438/1000 Iteration: 877 Train loss: 0.004518\n",
      "Epoch: 439/1000 Iteration: 879 Train loss: 0.004515\n",
      "Epoch: 440/1000 Iteration: 881 Train loss: 0.004512\n",
      "Epoch: 441/1000 Iteration: 883 Train loss: 0.004510\n",
      "Epoch: 442/1000 Iteration: 885 Train loss: 0.004507\n",
      "Epoch: 443/1000 Iteration: 887 Train loss: 0.004504\n",
      "Epoch: 444/1000 Iteration: 889 Train loss: 0.004501\n",
      "Epoch: 445/1000 Iteration: 891 Train loss: 0.004499\n",
      "Epoch: 446/1000 Iteration: 893 Train loss: 0.004496\n",
      "Epoch: 447/1000 Iteration: 895 Train loss: 0.004493\n",
      "Epoch: 448/1000 Iteration: 897 Train loss: 0.004491\n",
      "Epoch: 449/1000 Iteration: 899 Train loss: 0.004488\n",
      "Epoch: 450/1000 Iteration: 901 Train loss: 0.004486\n",
      "Epoch: 451/1000 Iteration: 903 Train loss: 0.004483\n",
      "Epoch: 452/1000 Iteration: 905 Train loss: 0.004481\n",
      "Epoch: 453/1000 Iteration: 907 Train loss: 0.004478\n",
      "Epoch: 454/1000 Iteration: 909 Train loss: 0.004476\n",
      "Epoch: 455/1000 Iteration: 911 Train loss: 0.004473\n",
      "Epoch: 456/1000 Iteration: 913 Train loss: 0.004471\n",
      "Epoch: 457/1000 Iteration: 915 Train loss: 0.004469\n",
      "Epoch: 458/1000 Iteration: 917 Train loss: 0.004466\n",
      "Epoch: 459/1000 Iteration: 919 Train loss: 0.004464\n",
      "Epoch: 460/1000 Iteration: 921 Train loss: 0.004462\n",
      "Epoch: 461/1000 Iteration: 923 Train loss: 0.004460\n",
      "Epoch: 462/1000 Iteration: 925 Train loss: 0.004457\n",
      "Epoch: 463/1000 Iteration: 927 Train loss: 0.004455\n",
      "Epoch: 464/1000 Iteration: 929 Train loss: 0.004453\n",
      "Epoch: 465/1000 Iteration: 931 Train loss: 0.004451\n",
      "Epoch: 466/1000 Iteration: 933 Train loss: 0.004449\n",
      "Epoch: 467/1000 Iteration: 935 Train loss: 0.004447\n",
      "Epoch: 468/1000 Iteration: 937 Train loss: 0.004445\n",
      "Epoch: 469/1000 Iteration: 939 Train loss: 0.004443\n",
      "Epoch: 470/1000 Iteration: 941 Train loss: 0.004441\n",
      "Epoch: 471/1000 Iteration: 943 Train loss: 0.004439\n",
      "Epoch: 472/1000 Iteration: 945 Train loss: 0.004437\n",
      "Epoch: 473/1000 Iteration: 947 Train loss: 0.004435\n",
      "Epoch: 474/1000 Iteration: 949 Train loss: 0.004433\n",
      "Epoch: 475/1000 Iteration: 951 Train loss: 0.004431\n",
      "Epoch: 476/1000 Iteration: 953 Train loss: 0.004429\n",
      "Epoch: 477/1000 Iteration: 955 Train loss: 0.004427\n",
      "Epoch: 478/1000 Iteration: 957 Train loss: 0.004426\n",
      "Epoch: 479/1000 Iteration: 959 Train loss: 0.004424\n",
      "Epoch: 480/1000 Iteration: 961 Train loss: 0.004422\n",
      "Epoch: 481/1000 Iteration: 963 Train loss: 0.004420\n",
      "Epoch: 482/1000 Iteration: 965 Train loss: 0.004419\n",
      "Epoch: 483/1000 Iteration: 967 Train loss: 0.004417\n",
      "Epoch: 484/1000 Iteration: 969 Train loss: 0.004415\n",
      "Epoch: 485/1000 Iteration: 971 Train loss: 0.004414\n",
      "Epoch: 486/1000 Iteration: 973 Train loss: 0.004412\n",
      "Epoch: 487/1000 Iteration: 975 Train loss: 0.004410\n",
      "Epoch: 488/1000 Iteration: 977 Train loss: 0.004409\n",
      "Epoch: 489/1000 Iteration: 979 Train loss: 0.004407\n",
      "Epoch: 490/1000 Iteration: 981 Train loss: 0.004405\n",
      "Epoch: 491/1000 Iteration: 983 Train loss: 0.004404\n",
      "Epoch: 492/1000 Iteration: 985 Train loss: 0.004402\n",
      "Epoch: 493/1000 Iteration: 987 Train loss: 0.004401\n",
      "Epoch: 494/1000 Iteration: 989 Train loss: 0.004399\n",
      "Epoch: 495/1000 Iteration: 991 Train loss: 0.004398\n",
      "Epoch: 496/1000 Iteration: 993 Train loss: 0.004396\n",
      "Epoch: 497/1000 Iteration: 995 Train loss: 0.004395\n",
      "Epoch: 498/1000 Iteration: 997 Train loss: 0.004394\n",
      "Epoch: 499/1000 Iteration: 999 Train loss: 0.004392\n",
      "Epoch: 500/1000 Iteration: 1001 Train loss: 0.004391\n",
      "Epoch: 501/1000 Iteration: 1003 Train loss: 0.004389\n",
      "Epoch: 502/1000 Iteration: 1005 Train loss: 0.004388\n",
      "Epoch: 503/1000 Iteration: 1007 Train loss: 0.004387\n",
      "Epoch: 504/1000 Iteration: 1009 Train loss: 0.004385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 505/1000 Iteration: 1011 Train loss: 0.004384\n",
      "Epoch: 506/1000 Iteration: 1013 Train loss: 0.004383\n",
      "Epoch: 507/1000 Iteration: 1015 Train loss: 0.004381\n",
      "Epoch: 508/1000 Iteration: 1017 Train loss: 0.004380\n",
      "Epoch: 509/1000 Iteration: 1019 Train loss: 0.004379\n",
      "Epoch: 510/1000 Iteration: 1021 Train loss: 0.004378\n",
      "Epoch: 511/1000 Iteration: 1023 Train loss: 0.004376\n",
      "Epoch: 512/1000 Iteration: 1025 Train loss: 0.004375\n",
      "Epoch: 513/1000 Iteration: 1027 Train loss: 0.004374\n",
      "Epoch: 514/1000 Iteration: 1029 Train loss: 0.004373\n",
      "Epoch: 515/1000 Iteration: 1031 Train loss: 0.004371\n",
      "Epoch: 516/1000 Iteration: 1033 Train loss: 0.004370\n",
      "Epoch: 517/1000 Iteration: 1035 Train loss: 0.004369\n",
      "Epoch: 518/1000 Iteration: 1037 Train loss: 0.004368\n",
      "Epoch: 519/1000 Iteration: 1039 Train loss: 0.004367\n",
      "Epoch: 520/1000 Iteration: 1041 Train loss: 0.004366\n",
      "Epoch: 521/1000 Iteration: 1043 Train loss: 0.004365\n",
      "Epoch: 522/1000 Iteration: 1045 Train loss: 0.004363\n",
      "Epoch: 523/1000 Iteration: 1047 Train loss: 0.004362\n",
      "Epoch: 524/1000 Iteration: 1049 Train loss: 0.004361\n",
      "Epoch: 525/1000 Iteration: 1051 Train loss: 0.004360\n",
      "Epoch: 526/1000 Iteration: 1053 Train loss: 0.004359\n",
      "Epoch: 527/1000 Iteration: 1055 Train loss: 0.004358\n",
      "Epoch: 528/1000 Iteration: 1057 Train loss: 0.004357\n",
      "Epoch: 529/1000 Iteration: 1059 Train loss: 0.004356\n",
      "Epoch: 530/1000 Iteration: 1061 Train loss: 0.004355\n",
      "Epoch: 531/1000 Iteration: 1063 Train loss: 0.004354\n",
      "Epoch: 532/1000 Iteration: 1065 Train loss: 0.004353\n",
      "Epoch: 533/1000 Iteration: 1067 Train loss: 0.004352\n",
      "Epoch: 534/1000 Iteration: 1069 Train loss: 0.004351\n",
      "Epoch: 535/1000 Iteration: 1071 Train loss: 0.004350\n",
      "Epoch: 536/1000 Iteration: 1073 Train loss: 0.004349\n",
      "Epoch: 537/1000 Iteration: 1075 Train loss: 0.004348\n",
      "Epoch: 538/1000 Iteration: 1077 Train loss: 0.004347\n",
      "Epoch: 539/1000 Iteration: 1079 Train loss: 0.004346\n",
      "Epoch: 540/1000 Iteration: 1081 Train loss: 0.004345\n",
      "Epoch: 541/1000 Iteration: 1083 Train loss: 0.004344\n",
      "Epoch: 542/1000 Iteration: 1085 Train loss: 0.004344\n",
      "Epoch: 543/1000 Iteration: 1087 Train loss: 0.004343\n",
      "Epoch: 544/1000 Iteration: 1089 Train loss: 0.004342\n",
      "Epoch: 545/1000 Iteration: 1091 Train loss: 0.004341\n",
      "Epoch: 546/1000 Iteration: 1093 Train loss: 0.004340\n",
      "Epoch: 547/1000 Iteration: 1095 Train loss: 0.004339\n",
      "Epoch: 548/1000 Iteration: 1097 Train loss: 0.004338\n",
      "Epoch: 549/1000 Iteration: 1099 Train loss: 0.004337\n",
      "Epoch: 550/1000 Iteration: 1101 Train loss: 0.004337\n",
      "Epoch: 551/1000 Iteration: 1103 Train loss: 0.004336\n",
      "Epoch: 552/1000 Iteration: 1105 Train loss: 0.004335\n",
      "Epoch: 553/1000 Iteration: 1107 Train loss: 0.004334\n",
      "Epoch: 554/1000 Iteration: 1109 Train loss: 0.004333\n",
      "Epoch: 555/1000 Iteration: 1111 Train loss: 0.004332\n",
      "Epoch: 556/1000 Iteration: 1113 Train loss: 0.004332\n",
      "Epoch: 557/1000 Iteration: 1115 Train loss: 0.004331\n",
      "Epoch: 558/1000 Iteration: 1117 Train loss: 0.004330\n",
      "Epoch: 559/1000 Iteration: 1119 Train loss: 0.004329\n",
      "Epoch: 560/1000 Iteration: 1121 Train loss: 0.004328\n",
      "Epoch: 561/1000 Iteration: 1123 Train loss: 0.004328\n",
      "Epoch: 562/1000 Iteration: 1125 Train loss: 0.004327\n",
      "Epoch: 563/1000 Iteration: 1127 Train loss: 0.004326\n",
      "Epoch: 564/1000 Iteration: 1129 Train loss: 0.004325\n",
      "Epoch: 565/1000 Iteration: 1131 Train loss: 0.004325\n",
      "Epoch: 566/1000 Iteration: 1133 Train loss: 0.004324\n",
      "Epoch: 567/1000 Iteration: 1135 Train loss: 0.004323\n",
      "Epoch: 568/1000 Iteration: 1137 Train loss: 0.004322\n",
      "Epoch: 569/1000 Iteration: 1139 Train loss: 0.004322\n",
      "Epoch: 570/1000 Iteration: 1141 Train loss: 0.004321\n",
      "Epoch: 571/1000 Iteration: 1143 Train loss: 0.004320\n",
      "Epoch: 572/1000 Iteration: 1145 Train loss: 0.004320\n",
      "Epoch: 573/1000 Iteration: 1147 Train loss: 0.004319\n",
      "Epoch: 574/1000 Iteration: 1149 Train loss: 0.004318\n",
      "Epoch: 575/1000 Iteration: 1151 Train loss: 0.004317\n",
      "Epoch: 576/1000 Iteration: 1153 Train loss: 0.004317\n",
      "Epoch: 577/1000 Iteration: 1155 Train loss: 0.004316\n",
      "Epoch: 578/1000 Iteration: 1157 Train loss: 0.004315\n",
      "Epoch: 579/1000 Iteration: 1159 Train loss: 0.004315\n",
      "Epoch: 580/1000 Iteration: 1161 Train loss: 0.004314\n",
      "Epoch: 581/1000 Iteration: 1163 Train loss: 0.004313\n",
      "Epoch: 582/1000 Iteration: 1165 Train loss: 0.004313\n",
      "Epoch: 583/1000 Iteration: 1167 Train loss: 0.004312\n",
      "Epoch: 584/1000 Iteration: 1169 Train loss: 0.004311\n",
      "Epoch: 585/1000 Iteration: 1171 Train loss: 0.004311\n",
      "Epoch: 586/1000 Iteration: 1173 Train loss: 0.004310\n",
      "Epoch: 587/1000 Iteration: 1175 Train loss: 0.004309\n",
      "Epoch: 588/1000 Iteration: 1177 Train loss: 0.004309\n",
      "Epoch: 589/1000 Iteration: 1179 Train loss: 0.004308\n",
      "Epoch: 590/1000 Iteration: 1181 Train loss: 0.004308\n",
      "Epoch: 591/1000 Iteration: 1183 Train loss: 0.004307\n",
      "Epoch: 592/1000 Iteration: 1185 Train loss: 0.004306\n",
      "Epoch: 593/1000 Iteration: 1187 Train loss: 0.004306\n",
      "Epoch: 594/1000 Iteration: 1189 Train loss: 0.004305\n",
      "Epoch: 595/1000 Iteration: 1191 Train loss: 0.004304\n",
      "Epoch: 596/1000 Iteration: 1193 Train loss: 0.004304\n",
      "Epoch: 597/1000 Iteration: 1195 Train loss: 0.004303\n",
      "Epoch: 598/1000 Iteration: 1197 Train loss: 0.004303\n",
      "Epoch: 599/1000 Iteration: 1199 Train loss: 0.004302\n",
      "Epoch: 600/1000 Iteration: 1201 Train loss: 0.004301\n",
      "Epoch: 601/1000 Iteration: 1203 Train loss: 0.004301\n",
      "Epoch: 602/1000 Iteration: 1205 Train loss: 0.004300\n",
      "Epoch: 603/1000 Iteration: 1207 Train loss: 0.004300\n",
      "Epoch: 604/1000 Iteration: 1209 Train loss: 0.004299\n",
      "Epoch: 605/1000 Iteration: 1211 Train loss: 0.004298\n",
      "Epoch: 606/1000 Iteration: 1213 Train loss: 0.004298\n",
      "Epoch: 607/1000 Iteration: 1215 Train loss: 0.004297\n",
      "Epoch: 608/1000 Iteration: 1217 Train loss: 0.004297\n",
      "Epoch: 609/1000 Iteration: 1219 Train loss: 0.004296\n",
      "Epoch: 610/1000 Iteration: 1221 Train loss: 0.004296\n",
      "Epoch: 611/1000 Iteration: 1223 Train loss: 0.004295\n",
      "Epoch: 612/1000 Iteration: 1225 Train loss: 0.004294\n",
      "Epoch: 613/1000 Iteration: 1227 Train loss: 0.004294\n",
      "Epoch: 614/1000 Iteration: 1229 Train loss: 0.004293\n",
      "Epoch: 615/1000 Iteration: 1231 Train loss: 0.004293\n",
      "Epoch: 616/1000 Iteration: 1233 Train loss: 0.004292\n",
      "Epoch: 617/1000 Iteration: 1235 Train loss: 0.004292\n",
      "Epoch: 618/1000 Iteration: 1237 Train loss: 0.004291\n",
      "Epoch: 619/1000 Iteration: 1239 Train loss: 0.004291\n",
      "Epoch: 620/1000 Iteration: 1241 Train loss: 0.004290\n",
      "Epoch: 621/1000 Iteration: 1243 Train loss: 0.004289\n",
      "Epoch: 622/1000 Iteration: 1245 Train loss: 0.004289\n",
      "Epoch: 623/1000 Iteration: 1247 Train loss: 0.004288\n",
      "Epoch: 624/1000 Iteration: 1249 Train loss: 0.004288\n",
      "Epoch: 625/1000 Iteration: 1251 Train loss: 0.004287\n",
      "Epoch: 626/1000 Iteration: 1253 Train loss: 0.004287\n",
      "Epoch: 627/1000 Iteration: 1255 Train loss: 0.004286\n",
      "Epoch: 628/1000 Iteration: 1257 Train loss: 0.004286\n",
      "Epoch: 629/1000 Iteration: 1259 Train loss: 0.004285\n",
      "Epoch: 630/1000 Iteration: 1261 Train loss: 0.004285\n",
      "Epoch: 631/1000 Iteration: 1263 Train loss: 0.004284\n",
      "Epoch: 632/1000 Iteration: 1265 Train loss: 0.004284\n",
      "Epoch: 633/1000 Iteration: 1267 Train loss: 0.004283\n",
      "Epoch: 634/1000 Iteration: 1269 Train loss: 0.004283\n",
      "Epoch: 635/1000 Iteration: 1271 Train loss: 0.004282\n",
      "Epoch: 636/1000 Iteration: 1273 Train loss: 0.004282\n",
      "Epoch: 637/1000 Iteration: 1275 Train loss: 0.004281\n",
      "Epoch: 638/1000 Iteration: 1277 Train loss: 0.004281\n",
      "Epoch: 639/1000 Iteration: 1279 Train loss: 0.004280\n",
      "Epoch: 640/1000 Iteration: 1281 Train loss: 0.004280\n",
      "Epoch: 641/1000 Iteration: 1283 Train loss: 0.004279\n",
      "Epoch: 642/1000 Iteration: 1285 Train loss: 0.004279\n",
      "Epoch: 643/1000 Iteration: 1287 Train loss: 0.004278\n",
      "Epoch: 644/1000 Iteration: 1289 Train loss: 0.004278\n",
      "Epoch: 645/1000 Iteration: 1291 Train loss: 0.004277\n",
      "Epoch: 646/1000 Iteration: 1293 Train loss: 0.004277\n",
      "Epoch: 647/1000 Iteration: 1295 Train loss: 0.004276\n",
      "Epoch: 648/1000 Iteration: 1297 Train loss: 0.004276\n",
      "Epoch: 649/1000 Iteration: 1299 Train loss: 0.004275\n",
      "Epoch: 650/1000 Iteration: 1301 Train loss: 0.004275\n",
      "Epoch: 651/1000 Iteration: 1303 Train loss: 0.004274\n",
      "Epoch: 652/1000 Iteration: 1305 Train loss: 0.004274\n",
      "Epoch: 653/1000 Iteration: 1307 Train loss: 0.004273\n",
      "Epoch: 654/1000 Iteration: 1309 Train loss: 0.004273\n",
      "Epoch: 655/1000 Iteration: 1311 Train loss: 0.004272\n",
      "Epoch: 656/1000 Iteration: 1313 Train loss: 0.004272\n",
      "Epoch: 657/1000 Iteration: 1315 Train loss: 0.004271\n",
      "Epoch: 658/1000 Iteration: 1317 Train loss: 0.004271\n",
      "Epoch: 659/1000 Iteration: 1319 Train loss: 0.004270\n",
      "Epoch: 660/1000 Iteration: 1321 Train loss: 0.004270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 661/1000 Iteration: 1323 Train loss: 0.004269\n",
      "Epoch: 662/1000 Iteration: 1325 Train loss: 0.004269\n",
      "Epoch: 663/1000 Iteration: 1327 Train loss: 0.004268\n",
      "Epoch: 664/1000 Iteration: 1329 Train loss: 0.004268\n",
      "Epoch: 665/1000 Iteration: 1331 Train loss: 0.004267\n",
      "Epoch: 666/1000 Iteration: 1333 Train loss: 0.004267\n",
      "Epoch: 667/1000 Iteration: 1335 Train loss: 0.004266\n",
      "Epoch: 668/1000 Iteration: 1337 Train loss: 0.004266\n",
      "Epoch: 669/1000 Iteration: 1339 Train loss: 0.004265\n",
      "Epoch: 670/1000 Iteration: 1341 Train loss: 0.004265\n",
      "Epoch: 671/1000 Iteration: 1343 Train loss: 0.004264\n",
      "Epoch: 672/1000 Iteration: 1345 Train loss: 0.004264\n",
      "Epoch: 673/1000 Iteration: 1347 Train loss: 0.004263\n",
      "Epoch: 674/1000 Iteration: 1349 Train loss: 0.004263\n",
      "Epoch: 675/1000 Iteration: 1351 Train loss: 0.004262\n",
      "Epoch: 676/1000 Iteration: 1353 Train loss: 0.004262\n",
      "Epoch: 677/1000 Iteration: 1355 Train loss: 0.004262\n",
      "Epoch: 678/1000 Iteration: 1357 Train loss: 0.004261\n",
      "Epoch: 679/1000 Iteration: 1359 Train loss: 0.004261\n",
      "Epoch: 680/1000 Iteration: 1361 Train loss: 0.004260\n",
      "Epoch: 681/1000 Iteration: 1363 Train loss: 0.004260\n",
      "Epoch: 682/1000 Iteration: 1365 Train loss: 0.004259\n",
      "Epoch: 683/1000 Iteration: 1367 Train loss: 0.004259\n",
      "Epoch: 684/1000 Iteration: 1369 Train loss: 0.004258\n",
      "Epoch: 685/1000 Iteration: 1371 Train loss: 0.004258\n",
      "Epoch: 686/1000 Iteration: 1373 Train loss: 0.004257\n",
      "Epoch: 687/1000 Iteration: 1375 Train loss: 0.004257\n",
      "Epoch: 688/1000 Iteration: 1377 Train loss: 0.004256\n",
      "Epoch: 689/1000 Iteration: 1379 Train loss: 0.004256\n",
      "Epoch: 690/1000 Iteration: 1381 Train loss: 0.004256\n",
      "Epoch: 691/1000 Iteration: 1383 Train loss: 0.004255\n",
      "Epoch: 692/1000 Iteration: 1385 Train loss: 0.004255\n",
      "Epoch: 693/1000 Iteration: 1387 Train loss: 0.004254\n",
      "Epoch: 694/1000 Iteration: 1389 Train loss: 0.004254\n",
      "Epoch: 695/1000 Iteration: 1391 Train loss: 0.004253\n",
      "Epoch: 696/1000 Iteration: 1393 Train loss: 0.004253\n",
      "Epoch: 697/1000 Iteration: 1395 Train loss: 0.004252\n",
      "Epoch: 698/1000 Iteration: 1397 Train loss: 0.004252\n",
      "Epoch: 699/1000 Iteration: 1399 Train loss: 0.004251\n",
      "Epoch: 700/1000 Iteration: 1401 Train loss: 0.004251\n",
      "Epoch: 701/1000 Iteration: 1403 Train loss: 0.004250\n",
      "Epoch: 702/1000 Iteration: 1405 Train loss: 0.004250\n",
      "Epoch: 703/1000 Iteration: 1407 Train loss: 0.004250\n",
      "Epoch: 704/1000 Iteration: 1409 Train loss: 0.004249\n",
      "Epoch: 705/1000 Iteration: 1411 Train loss: 0.004249\n",
      "Epoch: 706/1000 Iteration: 1413 Train loss: 0.004248\n",
      "Epoch: 707/1000 Iteration: 1415 Train loss: 0.004248\n",
      "Epoch: 708/1000 Iteration: 1417 Train loss: 0.004247\n",
      "Epoch: 709/1000 Iteration: 1419 Train loss: 0.004247\n",
      "Epoch: 710/1000 Iteration: 1421 Train loss: 0.004246\n",
      "Epoch: 711/1000 Iteration: 1423 Train loss: 0.004246\n",
      "Epoch: 712/1000 Iteration: 1425 Train loss: 0.004245\n",
      "Epoch: 713/1000 Iteration: 1427 Train loss: 0.004245\n",
      "Epoch: 714/1000 Iteration: 1429 Train loss: 0.004245\n",
      "Epoch: 715/1000 Iteration: 1431 Train loss: 0.004244\n",
      "Epoch: 716/1000 Iteration: 1433 Train loss: 0.004244\n",
      "Epoch: 717/1000 Iteration: 1435 Train loss: 0.004243\n",
      "Epoch: 718/1000 Iteration: 1437 Train loss: 0.004243\n",
      "Epoch: 719/1000 Iteration: 1439 Train loss: 0.004242\n",
      "Epoch: 720/1000 Iteration: 1441 Train loss: 0.004242\n",
      "Epoch: 721/1000 Iteration: 1443 Train loss: 0.004241\n",
      "Epoch: 722/1000 Iteration: 1445 Train loss: 0.004241\n",
      "Epoch: 723/1000 Iteration: 1447 Train loss: 0.004241\n",
      "Epoch: 724/1000 Iteration: 1449 Train loss: 0.004240\n",
      "Epoch: 725/1000 Iteration: 1451 Train loss: 0.004240\n",
      "Epoch: 726/1000 Iteration: 1453 Train loss: 0.004239\n",
      "Epoch: 727/1000 Iteration: 1455 Train loss: 0.004239\n",
      "Epoch: 728/1000 Iteration: 1457 Train loss: 0.004238\n",
      "Epoch: 729/1000 Iteration: 1459 Train loss: 0.004238\n",
      "Epoch: 730/1000 Iteration: 1461 Train loss: 0.004237\n",
      "Epoch: 731/1000 Iteration: 1463 Train loss: 0.004237\n",
      "Epoch: 732/1000 Iteration: 1465 Train loss: 0.004236\n",
      "Epoch: 733/1000 Iteration: 1467 Train loss: 0.004236\n",
      "Epoch: 734/1000 Iteration: 1469 Train loss: 0.004236\n",
      "Epoch: 735/1000 Iteration: 1471 Train loss: 0.004235\n",
      "Epoch: 736/1000 Iteration: 1473 Train loss: 0.004235\n",
      "Epoch: 737/1000 Iteration: 1475 Train loss: 0.004234\n",
      "Epoch: 738/1000 Iteration: 1477 Train loss: 0.004234\n",
      "Epoch: 739/1000 Iteration: 1479 Train loss: 0.004233\n",
      "Epoch: 740/1000 Iteration: 1481 Train loss: 0.004233\n",
      "Epoch: 741/1000 Iteration: 1483 Train loss: 0.004232\n",
      "Epoch: 742/1000 Iteration: 1485 Train loss: 0.004232\n",
      "Epoch: 743/1000 Iteration: 1487 Train loss: 0.004232\n",
      "Epoch: 744/1000 Iteration: 1489 Train loss: 0.004231\n",
      "Epoch: 745/1000 Iteration: 1491 Train loss: 0.004231\n",
      "Epoch: 746/1000 Iteration: 1493 Train loss: 0.004230\n",
      "Epoch: 747/1000 Iteration: 1495 Train loss: 0.004230\n",
      "Epoch: 748/1000 Iteration: 1497 Train loss: 0.004229\n",
      "Epoch: 749/1000 Iteration: 1499 Train loss: 0.004229\n",
      "Epoch: 750/1000 Iteration: 1501 Train loss: 0.004228\n",
      "Epoch: 751/1000 Iteration: 1503 Train loss: 0.004228\n",
      "Epoch: 752/1000 Iteration: 1505 Train loss: 0.004228\n",
      "Epoch: 753/1000 Iteration: 1507 Train loss: 0.004227\n",
      "Epoch: 754/1000 Iteration: 1509 Train loss: 0.004227\n",
      "Epoch: 755/1000 Iteration: 1511 Train loss: 0.004226\n",
      "Epoch: 756/1000 Iteration: 1513 Train loss: 0.004226\n",
      "Epoch: 757/1000 Iteration: 1515 Train loss: 0.004225\n",
      "Epoch: 758/1000 Iteration: 1517 Train loss: 0.004225\n",
      "Epoch: 759/1000 Iteration: 1519 Train loss: 0.004224\n",
      "Epoch: 760/1000 Iteration: 1521 Train loss: 0.004224\n",
      "Epoch: 761/1000 Iteration: 1523 Train loss: 0.004224\n",
      "Epoch: 762/1000 Iteration: 1525 Train loss: 0.004223\n",
      "Epoch: 763/1000 Iteration: 1527 Train loss: 0.004223\n",
      "Epoch: 764/1000 Iteration: 1529 Train loss: 0.004222\n",
      "Epoch: 765/1000 Iteration: 1531 Train loss: 0.004222\n",
      "Epoch: 766/1000 Iteration: 1533 Train loss: 0.004221\n",
      "Epoch: 767/1000 Iteration: 1535 Train loss: 0.004221\n",
      "Epoch: 768/1000 Iteration: 1537 Train loss: 0.004220\n",
      "Epoch: 769/1000 Iteration: 1539 Train loss: 0.004220\n",
      "Epoch: 770/1000 Iteration: 1541 Train loss: 0.004220\n",
      "Epoch: 771/1000 Iteration: 1543 Train loss: 0.004219\n",
      "Epoch: 772/1000 Iteration: 1545 Train loss: 0.004219\n",
      "Epoch: 773/1000 Iteration: 1547 Train loss: 0.004218\n",
      "Epoch: 774/1000 Iteration: 1549 Train loss: 0.004218\n",
      "Epoch: 775/1000 Iteration: 1551 Train loss: 0.004217\n",
      "Epoch: 776/1000 Iteration: 1553 Train loss: 0.004217\n",
      "Epoch: 777/1000 Iteration: 1555 Train loss: 0.004216\n",
      "Epoch: 778/1000 Iteration: 1557 Train loss: 0.004216\n",
      "Epoch: 779/1000 Iteration: 1559 Train loss: 0.004216\n",
      "Epoch: 780/1000 Iteration: 1561 Train loss: 0.004215\n",
      "Epoch: 781/1000 Iteration: 1563 Train loss: 0.004215\n",
      "Epoch: 782/1000 Iteration: 1565 Train loss: 0.004214\n",
      "Epoch: 783/1000 Iteration: 1567 Train loss: 0.004214\n",
      "Epoch: 784/1000 Iteration: 1569 Train loss: 0.004213\n",
      "Epoch: 785/1000 Iteration: 1571 Train loss: 0.004213\n",
      "Epoch: 786/1000 Iteration: 1573 Train loss: 0.004212\n",
      "Epoch: 787/1000 Iteration: 1575 Train loss: 0.004212\n",
      "Epoch: 788/1000 Iteration: 1577 Train loss: 0.004212\n",
      "Epoch: 789/1000 Iteration: 1579 Train loss: 0.004211\n",
      "Epoch: 790/1000 Iteration: 1581 Train loss: 0.004211\n",
      "Epoch: 791/1000 Iteration: 1583 Train loss: 0.004210\n",
      "Epoch: 792/1000 Iteration: 1585 Train loss: 0.004210\n",
      "Epoch: 793/1000 Iteration: 1587 Train loss: 0.004209\n",
      "Epoch: 794/1000 Iteration: 1589 Train loss: 0.004209\n",
      "Epoch: 795/1000 Iteration: 1591 Train loss: 0.004208\n",
      "Epoch: 796/1000 Iteration: 1593 Train loss: 0.004208\n",
      "Epoch: 797/1000 Iteration: 1595 Train loss: 0.004208\n",
      "Epoch: 798/1000 Iteration: 1597 Train loss: 0.004207\n",
      "Epoch: 799/1000 Iteration: 1599 Train loss: 0.004207\n",
      "Epoch: 800/1000 Iteration: 1601 Train loss: 0.004206\n",
      "Epoch: 801/1000 Iteration: 1603 Train loss: 0.004206\n",
      "Epoch: 802/1000 Iteration: 1605 Train loss: 0.004205\n",
      "Epoch: 803/1000 Iteration: 1607 Train loss: 0.004205\n",
      "Epoch: 804/1000 Iteration: 1609 Train loss: 0.004204\n",
      "Epoch: 805/1000 Iteration: 1611 Train loss: 0.004204\n",
      "Epoch: 806/1000 Iteration: 1613 Train loss: 0.004203\n",
      "Epoch: 807/1000 Iteration: 1615 Train loss: 0.004203\n",
      "Epoch: 808/1000 Iteration: 1617 Train loss: 0.004203\n",
      "Epoch: 809/1000 Iteration: 1619 Train loss: 0.004202\n",
      "Epoch: 810/1000 Iteration: 1621 Train loss: 0.004202\n",
      "Epoch: 811/1000 Iteration: 1623 Train loss: 0.004201\n",
      "Epoch: 812/1000 Iteration: 1625 Train loss: 0.004201\n",
      "Epoch: 813/1000 Iteration: 1627 Train loss: 0.004200\n",
      "Epoch: 814/1000 Iteration: 1629 Train loss: 0.004200\n",
      "Epoch: 815/1000 Iteration: 1631 Train loss: 0.004199\n",
      "Epoch: 816/1000 Iteration: 1633 Train loss: 0.004199\n",
      "Epoch: 817/1000 Iteration: 1635 Train loss: 0.004199\n",
      "Epoch: 818/1000 Iteration: 1637 Train loss: 0.004198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 819/1000 Iteration: 1639 Train loss: 0.004198\n",
      "Epoch: 820/1000 Iteration: 1641 Train loss: 0.004197\n",
      "Epoch: 821/1000 Iteration: 1643 Train loss: 0.004197\n",
      "Epoch: 822/1000 Iteration: 1645 Train loss: 0.004196\n",
      "Epoch: 823/1000 Iteration: 1647 Train loss: 0.004196\n",
      "Epoch: 824/1000 Iteration: 1649 Train loss: 0.004195\n",
      "Epoch: 825/1000 Iteration: 1651 Train loss: 0.004195\n",
      "Epoch: 826/1000 Iteration: 1653 Train loss: 0.004194\n",
      "Epoch: 827/1000 Iteration: 1655 Train loss: 0.004194\n",
      "Epoch: 828/1000 Iteration: 1657 Train loss: 0.004194\n",
      "Epoch: 829/1000 Iteration: 1659 Train loss: 0.004193\n",
      "Epoch: 830/1000 Iteration: 1661 Train loss: 0.004193\n",
      "Epoch: 831/1000 Iteration: 1663 Train loss: 0.004192\n",
      "Epoch: 832/1000 Iteration: 1665 Train loss: 0.004192\n",
      "Epoch: 833/1000 Iteration: 1667 Train loss: 0.004191\n",
      "Epoch: 834/1000 Iteration: 1669 Train loss: 0.004191\n",
      "Epoch: 835/1000 Iteration: 1671 Train loss: 0.004190\n",
      "Epoch: 836/1000 Iteration: 1673 Train loss: 0.004190\n",
      "Epoch: 837/1000 Iteration: 1675 Train loss: 0.004190\n",
      "Epoch: 838/1000 Iteration: 1677 Train loss: 0.004189\n",
      "Epoch: 839/1000 Iteration: 1679 Train loss: 0.004189\n",
      "Epoch: 840/1000 Iteration: 1681 Train loss: 0.004188\n",
      "Epoch: 841/1000 Iteration: 1683 Train loss: 0.004188\n",
      "Epoch: 842/1000 Iteration: 1685 Train loss: 0.004187\n",
      "Epoch: 843/1000 Iteration: 1687 Train loss: 0.004187\n",
      "Epoch: 844/1000 Iteration: 1689 Train loss: 0.004186\n",
      "Epoch: 845/1000 Iteration: 1691 Train loss: 0.004186\n",
      "Epoch: 846/1000 Iteration: 1693 Train loss: 0.004185\n",
      "Epoch: 847/1000 Iteration: 1695 Train loss: 0.004185\n",
      "Epoch: 848/1000 Iteration: 1697 Train loss: 0.004185\n",
      "Epoch: 849/1000 Iteration: 1699 Train loss: 0.004184\n",
      "Epoch: 850/1000 Iteration: 1701 Train loss: 0.004184\n",
      "Epoch: 851/1000 Iteration: 1703 Train loss: 0.004183\n",
      "Epoch: 852/1000 Iteration: 1705 Train loss: 0.004183\n",
      "Epoch: 853/1000 Iteration: 1707 Train loss: 0.004182\n",
      "Epoch: 854/1000 Iteration: 1709 Train loss: 0.004182\n",
      "Epoch: 855/1000 Iteration: 1711 Train loss: 0.004181\n",
      "Epoch: 856/1000 Iteration: 1713 Train loss: 0.004181\n",
      "Epoch: 857/1000 Iteration: 1715 Train loss: 0.004180\n",
      "Epoch: 858/1000 Iteration: 1717 Train loss: 0.004180\n",
      "Epoch: 859/1000 Iteration: 1719 Train loss: 0.004180\n",
      "Epoch: 860/1000 Iteration: 1721 Train loss: 0.004179\n",
      "Epoch: 861/1000 Iteration: 1723 Train loss: 0.004179\n",
      "Epoch: 862/1000 Iteration: 1725 Train loss: 0.004178\n",
      "Epoch: 863/1000 Iteration: 1727 Train loss: 0.004178\n",
      "Epoch: 864/1000 Iteration: 1729 Train loss: 0.004177\n",
      "Epoch: 865/1000 Iteration: 1731 Train loss: 0.004177\n",
      "Epoch: 866/1000 Iteration: 1733 Train loss: 0.004176\n",
      "Epoch: 867/1000 Iteration: 1735 Train loss: 0.004176\n",
      "Epoch: 868/1000 Iteration: 1737 Train loss: 0.004175\n",
      "Epoch: 869/1000 Iteration: 1739 Train loss: 0.004175\n",
      "Epoch: 870/1000 Iteration: 1741 Train loss: 0.004174\n",
      "Epoch: 871/1000 Iteration: 1743 Train loss: 0.004174\n",
      "Epoch: 872/1000 Iteration: 1745 Train loss: 0.004174\n",
      "Epoch: 873/1000 Iteration: 1747 Train loss: 0.004173\n",
      "Epoch: 874/1000 Iteration: 1749 Train loss: 0.004173\n",
      "Epoch: 875/1000 Iteration: 1751 Train loss: 0.004172\n",
      "Epoch: 876/1000 Iteration: 1753 Train loss: 0.004172\n",
      "Epoch: 877/1000 Iteration: 1755 Train loss: 0.004171\n",
      "Epoch: 878/1000 Iteration: 1757 Train loss: 0.004171\n",
      "Epoch: 879/1000 Iteration: 1759 Train loss: 0.004170\n",
      "Epoch: 880/1000 Iteration: 1761 Train loss: 0.004170\n",
      "Epoch: 881/1000 Iteration: 1763 Train loss: 0.004169\n",
      "Epoch: 882/1000 Iteration: 1765 Train loss: 0.004169\n",
      "Epoch: 883/1000 Iteration: 1767 Train loss: 0.004169\n",
      "Epoch: 884/1000 Iteration: 1769 Train loss: 0.004168\n",
      "Epoch: 885/1000 Iteration: 1771 Train loss: 0.004168\n",
      "Epoch: 886/1000 Iteration: 1773 Train loss: 0.004167\n",
      "Epoch: 887/1000 Iteration: 1775 Train loss: 0.004167\n",
      "Epoch: 888/1000 Iteration: 1777 Train loss: 0.004166\n",
      "Epoch: 889/1000 Iteration: 1779 Train loss: 0.004166\n",
      "Epoch: 890/1000 Iteration: 1781 Train loss: 0.004165\n",
      "Epoch: 891/1000 Iteration: 1783 Train loss: 0.004165\n",
      "Epoch: 892/1000 Iteration: 1785 Train loss: 0.004164\n",
      "Epoch: 893/1000 Iteration: 1787 Train loss: 0.004164\n",
      "Epoch: 894/1000 Iteration: 1789 Train loss: 0.004163\n",
      "Epoch: 895/1000 Iteration: 1791 Train loss: 0.004163\n",
      "Epoch: 896/1000 Iteration: 1793 Train loss: 0.004162\n",
      "Epoch: 897/1000 Iteration: 1795 Train loss: 0.004162\n",
      "Epoch: 898/1000 Iteration: 1797 Train loss: 0.004162\n",
      "Epoch: 899/1000 Iteration: 1799 Train loss: 0.004161\n",
      "Epoch: 900/1000 Iteration: 1801 Train loss: 0.004161\n",
      "Epoch: 901/1000 Iteration: 1803 Train loss: 0.004160\n",
      "Epoch: 902/1000 Iteration: 1805 Train loss: 0.004160\n",
      "Epoch: 903/1000 Iteration: 1807 Train loss: 0.004159\n",
      "Epoch: 904/1000 Iteration: 1809 Train loss: 0.004159\n",
      "Epoch: 905/1000 Iteration: 1811 Train loss: 0.004158\n",
      "Epoch: 906/1000 Iteration: 1813 Train loss: 0.004158\n",
      "Epoch: 907/1000 Iteration: 1815 Train loss: 0.004157\n",
      "Epoch: 908/1000 Iteration: 1817 Train loss: 0.004157\n",
      "Epoch: 909/1000 Iteration: 1819 Train loss: 0.004156\n",
      "Epoch: 910/1000 Iteration: 1821 Train loss: 0.004156\n",
      "Epoch: 911/1000 Iteration: 1823 Train loss: 0.004156\n",
      "Epoch: 912/1000 Iteration: 1825 Train loss: 0.004155\n",
      "Epoch: 913/1000 Iteration: 1827 Train loss: 0.004155\n",
      "Epoch: 914/1000 Iteration: 1829 Train loss: 0.004154\n",
      "Epoch: 915/1000 Iteration: 1831 Train loss: 0.004154\n",
      "Epoch: 916/1000 Iteration: 1833 Train loss: 0.004153\n",
      "Epoch: 917/1000 Iteration: 1835 Train loss: 0.004153\n",
      "Epoch: 918/1000 Iteration: 1837 Train loss: 0.004152\n",
      "Epoch: 919/1000 Iteration: 1839 Train loss: 0.004152\n",
      "Epoch: 920/1000 Iteration: 1841 Train loss: 0.004151\n",
      "Epoch: 921/1000 Iteration: 1843 Train loss: 0.004151\n",
      "Epoch: 922/1000 Iteration: 1845 Train loss: 0.004150\n",
      "Epoch: 923/1000 Iteration: 1847 Train loss: 0.004150\n",
      "Epoch: 924/1000 Iteration: 1849 Train loss: 0.004149\n",
      "Epoch: 925/1000 Iteration: 1851 Train loss: 0.004149\n",
      "Epoch: 926/1000 Iteration: 1853 Train loss: 0.004149\n",
      "Epoch: 927/1000 Iteration: 1855 Train loss: 0.004148\n",
      "Epoch: 928/1000 Iteration: 1857 Train loss: 0.004148\n",
      "Epoch: 929/1000 Iteration: 1859 Train loss: 0.004147\n",
      "Epoch: 930/1000 Iteration: 1861 Train loss: 0.004147\n",
      "Epoch: 931/1000 Iteration: 1863 Train loss: 0.004146\n",
      "Epoch: 932/1000 Iteration: 1865 Train loss: 0.004146\n",
      "Epoch: 933/1000 Iteration: 1867 Train loss: 0.004145\n",
      "Epoch: 934/1000 Iteration: 1869 Train loss: 0.004145\n",
      "Epoch: 935/1000 Iteration: 1871 Train loss: 0.004144\n",
      "Epoch: 936/1000 Iteration: 1873 Train loss: 0.004144\n",
      "Epoch: 937/1000 Iteration: 1875 Train loss: 0.004143\n",
      "Epoch: 938/1000 Iteration: 1877 Train loss: 0.004143\n",
      "Epoch: 939/1000 Iteration: 1879 Train loss: 0.004142\n",
      "Epoch: 940/1000 Iteration: 1881 Train loss: 0.004142\n",
      "Epoch: 941/1000 Iteration: 1883 Train loss: 0.004141\n",
      "Epoch: 942/1000 Iteration: 1885 Train loss: 0.004141\n",
      "Epoch: 943/1000 Iteration: 1887 Train loss: 0.004140\n",
      "Epoch: 944/1000 Iteration: 1889 Train loss: 0.004140\n",
      "Epoch: 945/1000 Iteration: 1891 Train loss: 0.004140\n",
      "Epoch: 946/1000 Iteration: 1893 Train loss: 0.004139\n",
      "Epoch: 947/1000 Iteration: 1895 Train loss: 0.004139\n",
      "Epoch: 948/1000 Iteration: 1897 Train loss: 0.004138\n",
      "Epoch: 949/1000 Iteration: 1899 Train loss: 0.004138\n",
      "Epoch: 950/1000 Iteration: 1901 Train loss: 0.004137\n",
      "Epoch: 951/1000 Iteration: 1903 Train loss: 0.004137\n",
      "Epoch: 952/1000 Iteration: 1905 Train loss: 0.004136\n",
      "Epoch: 953/1000 Iteration: 1907 Train loss: 0.004136\n",
      "Epoch: 954/1000 Iteration: 1909 Train loss: 0.004135\n",
      "Epoch: 955/1000 Iteration: 1911 Train loss: 0.004135\n",
      "Epoch: 956/1000 Iteration: 1913 Train loss: 0.004134\n",
      "Epoch: 957/1000 Iteration: 1915 Train loss: 0.004134\n",
      "Epoch: 958/1000 Iteration: 1917 Train loss: 0.004133\n",
      "Epoch: 959/1000 Iteration: 1919 Train loss: 0.004133\n",
      "Epoch: 960/1000 Iteration: 1921 Train loss: 0.004132\n",
      "Epoch: 961/1000 Iteration: 1923 Train loss: 0.004132\n",
      "Epoch: 962/1000 Iteration: 1925 Train loss: 0.004131\n",
      "Epoch: 963/1000 Iteration: 1927 Train loss: 0.004131\n",
      "Epoch: 964/1000 Iteration: 1929 Train loss: 0.004131\n",
      "Epoch: 965/1000 Iteration: 1931 Train loss: 0.004130\n",
      "Epoch: 966/1000 Iteration: 1933 Train loss: 0.004130\n",
      "Epoch: 967/1000 Iteration: 1935 Train loss: 0.004129\n",
      "Epoch: 968/1000 Iteration: 1937 Train loss: 0.004129\n",
      "Epoch: 969/1000 Iteration: 1939 Train loss: 0.004128\n",
      "Epoch: 970/1000 Iteration: 1941 Train loss: 0.004128\n",
      "Epoch: 971/1000 Iteration: 1943 Train loss: 0.004127\n",
      "Epoch: 972/1000 Iteration: 1945 Train loss: 0.004127\n",
      "Epoch: 973/1000 Iteration: 1947 Train loss: 0.004126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 974/1000 Iteration: 1949 Train loss: 0.004126\n",
      "Epoch: 975/1000 Iteration: 1951 Train loss: 0.004125\n",
      "Epoch: 976/1000 Iteration: 1953 Train loss: 0.004125\n",
      "Epoch: 977/1000 Iteration: 1955 Train loss: 0.004124\n",
      "Epoch: 978/1000 Iteration: 1957 Train loss: 0.004124\n",
      "Epoch: 979/1000 Iteration: 1959 Train loss: 0.004123\n",
      "Epoch: 980/1000 Iteration: 1961 Train loss: 0.004123\n",
      "Epoch: 981/1000 Iteration: 1963 Train loss: 0.004122\n",
      "Epoch: 982/1000 Iteration: 1965 Train loss: 0.004122\n",
      "Epoch: 983/1000 Iteration: 1967 Train loss: 0.004121\n",
      "Epoch: 984/1000 Iteration: 1969 Train loss: 0.004121\n",
      "Epoch: 985/1000 Iteration: 1971 Train loss: 0.004120\n",
      "Epoch: 986/1000 Iteration: 1973 Train loss: 0.004120\n",
      "Epoch: 987/1000 Iteration: 1975 Train loss: 0.004119\n",
      "Epoch: 988/1000 Iteration: 1977 Train loss: 0.004119\n",
      "Epoch: 989/1000 Iteration: 1979 Train loss: 0.004119\n",
      "Epoch: 990/1000 Iteration: 1981 Train loss: 0.004118\n",
      "Epoch: 991/1000 Iteration: 1983 Train loss: 0.004118\n",
      "Epoch: 992/1000 Iteration: 1985 Train loss: 0.004117\n",
      "Epoch: 993/1000 Iteration: 1987 Train loss: 0.004117\n",
      "Epoch: 994/1000 Iteration: 1989 Train loss: 0.004116\n",
      "Epoch: 995/1000 Iteration: 1991 Train loss: 0.004116\n",
      "Epoch: 996/1000 Iteration: 1993 Train loss: 0.004115\n",
      "Epoch: 997/1000 Iteration: 1995 Train loss: 0.004115\n",
      "Epoch: 998/1000 Iteration: 1997 Train loss: 0.004114\n",
      "Epoch: 999/1000 Iteration: 1999 Train loss: 0.004114\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b61b0c128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn_1/Reshape_1:0\", shape=(?, 5, 10), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from ./models/BusinessRisk-lstm/caiwu_predict.ckpt\n",
      "[[0.31339307363088215], [0.44824200150085197], [0.5147971952368775], [0.6276184487903889], [0.7437064711475722]]\n",
      "[[0.448242  ]\n",
      " [0.5147972 ]\n",
      " [0.62761845]\n",
      " [0.74370647]\n",
      " [0.90759605]]\n",
      "[[0.5147972 ]\n",
      " [0.62761845]\n",
      " [0.74370647]\n",
      " [0.90759605]\n",
      " [1.06792355]]\n",
      "[[0.62761845]\n",
      " [0.74370647]\n",
      " [0.90759605]\n",
      " [1.06792355]\n",
      " [1.23059642]]\n",
      "[[0.74370647]\n",
      " [0.90759605]\n",
      " [1.06792355]\n",
      " [1.23059642]\n",
      " [1.39269829]]\n",
      "预测值： [[6.39591808]\n",
      " [7.47966295]\n",
      " [8.57926159]\n",
      " [9.67500044]]\n",
      "真实值： [[5.92401827]\n",
      " [5.77068446]\n",
      " [6.84187   ]\n",
      " [7.02052901]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4ae77c77b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************************\n",
      "\n",
      "*********************************迭代结束*********************************\n",
      "\n",
      "**************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdate\n",
    "%matplotlib inline \n",
    "\n",
    "# 加载数据\n",
    "path = \"./dataSets/房地产行业财务风险数据.xlsx\"\n",
    "# skipfooter=3\n",
    "\n",
    "Matrix_pre = np.zeros(shape=(4,1))\n",
    "\n",
    "#print (Matrix_pre)\n",
    "\n",
    "for i in range(4,5):\n",
    "    \n",
    "    print('**************************************************************************\\n')\n",
    "    print('**************************当前是第'+str(i)+'列数据,共16列**************************\\n')\n",
    "    print('**************************************************************************\\n')\n",
    "\n",
    "    dataset = pd.read_excel(path, sheet_name = 'Sheet3', usecols= [i],  nrows = 20)\n",
    "    dataset = np.array(dataset)\n",
    "\n",
    "    print(dataset)\n",
    "\n",
    "    plt.plot(dataset)\n",
    "    plt.show()\n",
    "\n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataSet = scaler.fit_transform(dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # split into train and test sets; 80% 是训练数据，其余是测试数据\n",
    "    train_size = int(len(dataSet) * 0.8)\n",
    "    test_size = len(dataSet) - train_size\n",
    "    train, test = dataSet[0:train_size], dataSet[train_size:len(dataSet)]\n",
    "\n",
    "    # 数据格式转化(t,t+1)\n",
    "    def convert_data(data, time_step=1):\n",
    "        data_X,data_Y = [],[]  \n",
    "        for i in range(len(data) - time_step - 1):\n",
    "            x = data[i: (i + time_step)]  \n",
    "            y = data[i+1:i + time_step+1]      \n",
    "            data_X.append(x.tolist())\n",
    "            data_Y.append(y.tolist()) \n",
    "        return data_X, data_Y\n",
    "\n",
    "    # fix random seed for reproducibility\n",
    "    np.random.seed(7)\n",
    "\n",
    "    # use this function to prepare the train and test datasets for modeling\n",
    "    #time_step=5\n",
    "    time_step = 5    #时间步\n",
    "    train_x, train_y = convert_data(train, time_step)\n",
    "    test_x, test_y = convert_data(test, time_step)\n",
    "\n",
    "    #———————————————————形成训练集—————————————————————\n",
    "    #设置常量\n",
    "    hidden_unit = 10       #hidden layer units 记忆和储存过去状态的节点个数\n",
    "    batch_size = 4    #每一批次训练多少个样例\n",
    "    input_size = 1      #输入层维度\n",
    "    output_size = 1     #输出层维度\n",
    "    lr = 0.0001#学习率\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # LSTM 的 X 需要有这样的结构： [samples, time steps, features]，所以做一下变换\n",
    "    X = tf.placeholder(tf.float32, [None,time_step,input_size], name=\"inputs\")    #每批次输入网络的tensor\n",
    "    Y = tf.placeholder(tf.float32, [None,time_step,output_size], name=\"outputs\")   #每批次tensor对应的标签\n",
    "    # 输入层、输出层权重、偏置\n",
    "    with tf.name_scope('layer'):\n",
    "            with tf.name_scope('weights'):\n",
    "                weights={\n",
    "                         'in':tf.Variable(tf.random_normal([input_size,hidden_unit])),\n",
    "                         'out':tf.Variable(tf.random_normal([hidden_unit,1]))\n",
    "                         }\n",
    "            with tf.name_scope('biases'):\n",
    "                biases={\n",
    "                        'in':tf.Variable(tf.constant(0.1,shape=[hidden_unit,])),\n",
    "                        'out':tf.Variable(tf.constant(0.1,shape=[1,]))\n",
    "                        }\n",
    "\n",
    "    def lstm(batch):  #参数：输入网络批次数目\n",
    "\n",
    "        w_in = weights['in']\n",
    "        b_in = biases['in']\n",
    "        input = tf.reshape(X,[-1,input_size])  #需要将tensor转成2维进行计算，计算后的结果作为隐藏层的输入\n",
    "      \n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_unit) #10个节点\n",
    "        input_lstm = tf.matmul(input, w_in) + b_in\n",
    "        input_lstm = tf.reshape(input_lstm, [-1, time_step, hidden_unit])  #将tensor转成3维，作为lstm cell的输入      \n",
    "        print(input_lstm)\n",
    "        init_state = lstm_cell.zero_state(batch,dtype = tf.float32)\n",
    "        # output_rnn是记录lstm每个隐状态输出节点的结果，final_states是最后一个cell的结果，数据格式为tuple\n",
    "        output_rnn, final_states = tf.nn.dynamic_rnn(\n",
    "            lstm_cell, \n",
    "            input_lstm, \n",
    "            initial_state = init_state, \n",
    "            dtype = tf.float32) \n",
    "\n",
    "        output = tf.reshape(output_rnn, [-1, hidden_unit]) #  作为输出层的输入\n",
    "        w_out = weights['out']\n",
    "        b_out = biases['out']\n",
    "         # 预测数据\n",
    "        multi = tf.matmul(output, w_out)\n",
    "        pred = tf.add(multi, b_out, name='preds') \n",
    "        return pred, final_states\n",
    "\n",
    "    train_loss = []\n",
    "    def train_lstm():   \n",
    "        global batch_size\n",
    "        iteration = 1\n",
    "        epochs = 1000\n",
    "    #     with tf.variable_scope(\"sec_lstm\"):\n",
    "        pred, _ = lstm(batch_size)\n",
    "        # 损失函数\n",
    "        loss = tf.reduce_mean(tf.square(tf.reshape(pred,[-1])-tf.reshape(Y, [-1])))\n",
    "        #tf.summary.scalar('loss_function', loss)\n",
    "        train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        with tf.Session() as sess:\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # summaries合并\n",
    "            #merged = tf.summary.merge_all()    \n",
    "            # 写到指定的磁盘路径中\n",
    "            #train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "            # 重复训练5000次\n",
    "            for e in range(epochs):\n",
    "                step=0\n",
    "                start = 0\n",
    "                end = start + batch_size\n",
    "                while(end < len(train_x)):\n",
    "                    x = train_x[start:end]\n",
    "                    y = train_y[start:end]\n",
    "                    _,loss_ = sess.run([train_op, loss], feed_dict = {X: x, Y:y, keep_prob : 0.3})\n",
    "                    start += batch_size\n",
    "                    end = start + batch_size\n",
    "                    # 每10步保存一次参数\n",
    "                    if step% 10 == 0:                    \n",
    "                        print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                        \"Iteration: {:d}\".format(iteration),\n",
    "                        \"Train loss: {:6f}\".format(loss_))\n",
    "                        #train_writer.add_summary(summary, e);\n",
    "\n",
    "                    train_loss.append(loss_)\n",
    "                    iteration += 1  \n",
    "                    step += 1\n",
    "            saver.save(sess, \"./models/BusinessRisk-lstm/caiwu_predict.ckpt\")\n",
    "             # 保存二进制模型\n",
    "            builder = tf.saved_model.builder.SavedModelBuilder(\"./models/BusinessRiskModel/model_\"+ str(i))\n",
    "            builder.add_meta_graph_and_variables(sess, ['mytag'])\n",
    "            builder.save()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #绘训练过程指标图\n",
    "            t = np.arange(iteration - 1)\n",
    "            plt.figure(figsize = (9,6))\n",
    "            plt.plot(t, np.array(train_loss),  'r-')\n",
    "            plt.xlabel(\"iteration\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend(['train'], loc='upper right')\n",
    "            plt.show()        \n",
    "\n",
    "    with tf.variable_scope('rnn', reuse=tf.AUTO_REUSE):\n",
    "            train_lstm()\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import math\n",
    "\n",
    "    def prediction():\n",
    "\n",
    "            pred, _ = lstm(1)  # 预测时只输入[1,time_step,inputSize]的测试数据\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            #预测季度\n",
    "            pre_quarter = 4\n",
    "            with tf.Session() as sess:\n",
    "                # 参数恢复\n",
    "                module_file = tf.train.latest_checkpoint(\"./models/BusinessRisk-lstm/\")\n",
    "                saver.restore(sess, module_file)\n",
    "                # 取训练集最后一行为测试样本. shape=[1,time_step,inputSize]\n",
    "                prev_seq = train_x[-1]\n",
    "                print(prev_seq)\n",
    "                predict = []\n",
    "                # 得到之后10个季度的预测结果\n",
    "                for i in range(pre_quarter):\n",
    "                    next_seq = sess.run(pred,feed_dict={X:[prev_seq]})\n",
    "                    predict.append(next_seq[-1])   \n",
    "                    #每次得到最后一个时间步的预测结果，与之前的数据加在一起，形成新的测试样本\n",
    "                    #np.vstack()表示垂直（按照行顺序）的把数组给堆叠起来。\n",
    "                    prev_seq = np.vstack((prev_seq[1:],next_seq[-1]))\n",
    "                    print(prev_seq)\n",
    "                #得到实际预测值\n",
    "                predictY = scaler.inverse_transform(predict)\n",
    "\n",
    "                testY = scaler.inverse_transform(test)\n",
    "                print(\"预测值：\", predictY)\n",
    "                print(\"真实值：\",testY )\n",
    "\n",
    "\n",
    "                #Matrix_pre[0] = predictY\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #print (C)\n",
    "\n",
    "                global Matrix_pre\n",
    "                Matrix_pre = (np.hstack((Matrix_pre,predictY)))\n",
    "\n",
    "\n",
    "                #以折线图表示结果\n",
    "                plt.figure()\n",
    "                plt.title(\"lead index\")\n",
    "                plt.plot(list(range(len(testY))), testY, 'cx--', list(range(len(predict))), predictY, 'b--')\n",
    "                plt.xlabel(\"date-num\")\n",
    "                plt.ylabel(\"index\")\n",
    "                plt.legend(['train', 'pred'], loc='upper right')\n",
    "                plt.plot()\n",
    "                plt.show()\n",
    "\n",
    "    with tf.variable_scope('rnn', reuse = tf.AUTO_REUSE):\n",
    "        prediction() \n",
    "\n",
    "#存进excel文件\n",
    "Matrix_input = pd.DataFrame(Matrix_pre)\n",
    "writer = pd.ExcelWriter('./房地产行业财务风险预测数据.xlsx')\n",
    "Matrix_input.to_excel(writer, float_format='%.2f', header = False, index = False,) # float_format 控制精度\n",
    "writer.save()\n",
    "\n",
    "print('**************************************************************************\\n')\n",
    "print('*********************************迭代结束*********************************\\n')\n",
    "print('**************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
